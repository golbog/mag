**MoleculeNet: A Benchmark for Molecular Machine Learning** https://arxiv.org/pdf/1703.00564.pdf - The article describes MoleculeNet where they try to unify benchmarking standard for machine learning algorithms about molecular properties so that comparing between them is possible. They published their collected datasets, models and benchmarking results on http://moleculenet.ai which they also described in the article. Most importantly for us are the datasets that are prepared with SMILES strings and with output label with either 0/1 for classification. For each dataset, they state their recommended metric and spitting pattern that best fit the properties.

**Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules** https://arxiv.org/pdf/1610.02415.pdf - In this article, they tackle a similar problem we are trying to solve. Building an autoencoder for molecules represented in SMILES strings transformed into fixed length vector. To handle the problem of invalid decoding they used variational autoencoder so that resulting encoded latent space doesn't contain "empty spaces" that decode into invalid SMILES strings. They also constructed a multi-layer perceptron to predict the property of molecules from the latent vector. This all was used to build new molecules with the desired property. 
Architecture: Encoded canonical SMILES up to a maximum length of 120 characters for ZINC, padding the shorter strings.  For the autoencoder used for the ZINC dataset, the encoder used three 1D convolutional layers of filter sizes 9, 9, 10 and 9, 9, 11 convolution kernels, respectively, followed by one fully-connected layer of width 196. The decoder fed into three layers of a gated recurrent unit (GRU) networks with a hidden dimension of 488. For property prediction, two fully connected layers of 1000 neurons with a dropout rate of 0.2.
Evaluation: MAE prediction error comparring diferrent methods. 2D PCA visualization of encoded molecules.

**Grammar Variational Autoencoder** https://arxiv.org/pdf/1703.01925.pdf - They propose the grammar variational autoencoder (GVAE), using context-free grammar (CFG) to drastically reduce the number of invalid outputs generated by VAE without implicitly encouraging VAE to produce valid outputs. An encoder is built such that input SMILES string is formed into a parse tree based on SMILES grammar. The tree is then encoded into 1-hot vectors and fed to the encoder. Decoder works such that based on logits ensuring that returned sequence is valid, they keep track of the state of the parsing with LIFO stack always starting with a start symbol. 
Architecture: They used the same encoder and decoder as in the previous article with 56-dimensional latent space. The difference being the way they encode data as described above. 
Evaluation: They were mostly focused on the validity of the output of the decoder, so the fraction of valid outputs were used for evaluating.

**A Hierarchical Neural Autoencoder for Paragraphs and Documents** https://arxiv.org/pdf/1506.01057.pdf - They propose a hierarchical "long-short-term memory" (LSTM) autoencoder for paragraphs in natural language. In short, they split paragraphs into sentences with an end token and each sentence into words and end token. These words/tokens were embedded and fed to autoencoder with added LSTM layers for words and sentences, meaning words/sentences were encoded and decoded also based on the previous word's / sentences' hidden vector from LSTM model.
Architecture: LSTM structure with four layers for encoding and the same for decoding. Each LSTM layer consists of 1000 hidden neurons (same as the word embeddings dimensionality).
Evaluation: Closeness of the output compared to the input (ROUGE, BLEU, Coherence metrics).

**SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules** https://arxiv.org/pdf/1703.07076.pdf - Article states one of the problems in molecular machine learning is the lack of large labeled datasets. To solve this they make use of SMILES enumeration, which in contrast to canonical SMILES, many strings can code the same compound, thus expanding the dataset and improving results. The string was then padded to the max length in the dataset. Description of building the neural network was very scarce and they weren't building an autoencoder, but they used at least one LSTM layer in batch mode and the final state fed to a feed-forward neural network with a single linear output neuron.
Architecture: One layer of LSTM with 64 units fed into the feed-forward neural network.
Evaluation: They used correlation coefficient (0.66) and RMSE (0.55).


https://arxiv.org/pdf/1701.01329.pdf