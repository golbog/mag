**MoleculeNet: A Benchmark for Molecular Machine Learning** https://arxiv.org/pdf/1703.00564.pdf - Article describes MoleculeNet where they try to unify benchmarking standard for machine learning algorithms about molecular properties so that comparing between them is possible. They published their collected datasets, models and benchamrking results on http://moleculenet.ai which they also described in the article. Most importantly for us are the datasets that  are prepared with SMILES strings and with output label with either 0/1 for classification. For each dataset they state their recommended metric and spitting pattern that best fit the properties. 

**Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules** https://arxiv.org/pdf/1610.02415.pdf - In this article they tackle a similar problem we are trying to solve. Building an autoencoder for molecules represented in SMILES strings transformed into fixed length vector. To hande the problem of invalid decoding they used variational autoencoder, so that resulting encoded latent space doesn't contain "empty spaces" that decode into invalid SMILES strings. They also constructed a multi-layer perceptron to predict the property of molecules from the latent vector. This all was used to buid new molecules with a desired property. 

**Grammar Variational Autoencode** https://arxiv.org/pdf/1703.01925.pdf - They propose the grammar variational autoencoder (GVAE), using context-free grammar (CFG) to drastically reduce the number of invalid outputs generated by VAE without implicitly encouraging VAE to produce valid outputs. Encoder is built such that input SMILES string is formed into a parse tree based on SMILES grammar. Tree is then encoded into 1-hot vectors and fed to encoder. Decoder works such that based on logits ensuring that returned sequence is valid, they keep track of the state of the parsing with LIFO stack always starting with a start symbol. 

**A Hierarchical Neural Autoencoder for Paragraphs and Documents** https://arxiv.org/pdf/1506.01057.pdf - They propose a hierarchical "long-short term memory" (LSTM) autoencoder for paragraphs in natural language. In short they split paragraphs into sentences with an end token and each sentence into words and end token. This words/tokens were embedded and fed to autoencoder with added LSTM layers for words and sentences, meaning words/sentences were encoded and decoded also based on the previous word's/sentences' hidden vector from LSTM model.

**SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules** https://arxiv.org/pdf/1703.07076.pdf - Article states one of the problem in molecular machine learning is the lack of large labeled datasets. To solve this they make use of SMILES enumareation, which in constrast to canonical SMILES, many strings can code the same compound, thus expanding the dataset and improving results. String were then padded to the max length in dataset. Description of building the neural network was very scarse and weren't building an autoencoder, but they used at least one LSTM layer in batch mode and the final state fed to a feed-forward neural network with a single linear output neuron.